# 逐层量化特性说明

## 简介

逐层量化（Layer-wise Quantization）是modelslim_v1量化服务的重要特性，通过逐层处理模型来显著降低内存占用，使得大模型量化成为可能。

## 使用前准备

### 配置方法：启用逐层量化

```yaml
spec:
  runner: "layer_wise"  # 启用逐层量化
  process:
    - type: "linear_quant"
      qconfig:
        act:
          scope: "per_tensor"
          dtype: "int8"
          symmetric: false
          method: "minmax"
        weight:
          scope: "per_channel"
          dtype: "int8"
          symmetric: true
          method: "minmax"
      include: ["*"]
```

## 功能介绍

### 工作原理

| 量化方式 | 处理方式 | 内存占用 | 特点 | 适用场景 |
|----------|----------|----------|------|----------|
| 传统量化 | 模型级量化 | 模型大小的2-3倍 | 将整个模型加载到内存中处理 | 小模型（<7B） |
| 逐层量化 | 分层处理 | 单层大小的2-3倍 | 逐层加载参数，流水线操作 | 大模型（≥7B） |

### 核心优势

| 优势类型 | 传统方式 | 逐层方式 | 改进效果 |
|----------|----------|----------|----------|
| 内存占用 | 模型大小的2-3倍 | 单层大小的2-3倍 | 降低90%以上 |
| 模型支持 | 受内存限制 | 支持超大模型 | 支持任意大小模型 |
| 资源利用 | 固定内存需求 | 灵活适配硬件 | 更好的硬件适配性 |

### 使用场景

| 场景类型 | 场景描述 | 是否适用 | 说明 |
|----------|----------|----------|------|
| 大模型量化 | 7B及以上规模的模型 | ✅ 适用 | 内存优势明显 |
| 内存受限环境 | NPU内存不足的场景 | ✅ 适用 | 大幅降低内存需求 |


### 与其他特性的关系

| 特性类型 | 组件名称 | 兼容性 | 说明 |
|----------|----------|--------|------|
| 处理器 | linear_quant | ✅ 完全支持 | 完全支持逐层量化 |
| 处理器 | group处理器 | ✅ 支持 | 支持逐层处理 |
| 处理器 | 其他处理器 | ⚠️ 视情况而定 | 根据处理器特性决定 |
| 保存器 | ascendv1_saver | ✅ 支持 | 支持逐层保存 |
| 保存器 | mindie_format_saver | ✅ 支持多模态生成模型 | 支持逐层保存 |
| 保存器 | 其他保存器 | ⚠️ 视情况而定 | 根据保存器特性决定 |

### 模型适配

根据模型结构的差异，逐层量化可能需要一些适配工作，请参考[自主量化/模型接入](../../custom_quantization/integrating_models.md)完成模型接入，即可实现逐层量化的适配。

### 多模态理解模型支持

#### Qwen3-VL-MoE系列模型

Qwen3-VL-MoE系列模型（如Qwen3-VL-235B-A22B）支持逐层量化，通过以下方式实现：

- **模型结构**: 基于Transformer + MoE架构，视觉部分整体加载，文本部分逐层处理
- **MoE转换**: 逐层自动将3D融合权重转换为标准nn.Linear层，适配量化框架
- **量化策略**: 支持W8A8混合量化（MoE专家层动态量化 + 其他层静态量化）
- **内存优化**: 通过逐层量化显著降低内存占用，支持大规模MoE模型量化
- **配置示例**: 参考[qwen3_vl_moe_w8a8.yaml](../../../../lab_practice/qwen3_vl_moe/qwen3_vl_moe_w8a8.yaml)

**技术亮点**:
- 视觉部分一次性加载和量化，确保图像特征提取的完整性
- 文本部分采用逐层加载策略，按需加载每一层并立即进行量化处理
- MoE层自动识别并转换，无需手动干预

### 多模态生成模型支持

#### Wan2.1模型

Wan2.1模型支持逐层量化，通过以下方式实现：

- **模型结构**: 基于Transformer架构，支持逐层处理
- **量化策略**: 支持W8A8量化
- **内存优化**: 通过逐层量化显著降低内存占用，支持Atlas 800I/800T A2(64G)单卡量化
- **配置示例**: 参考[wan2_1_w8a8_dynamic.yaml](../../../../lab_practice/wan2_1/wan2_1_w8a8_dynamic.yaml)

### 其他多模态模型

后续将会逐步适配并提供指导。